{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus Part: Bayesian Statistics**\n",
    "\n",
    "Up until this point, we have been talking about the frequentists’ approach to statistical analysis, but frequentist is not the only school in modern statistical studies. While frequentists’ tools might be more useful in terms of club projects, I do want to talk about some of Bayesian’s tools as well. In my opinion, they are just as useful and versatile as some of the other frequentists’ tools we introduced in previous parts.\n",
    "\n",
    "As a statistician, one of the questions I got asked the most is: “Is frequentist better or bayesian better?” My answer, while not an appealing one, is the only correct answer to this question: “It depends.” Frequentist and Bayesian statistics differ by the underlying assumption and interpretation of the statistical world, specifically, how we describe the underlying parameter. Consider an unknown parameter theta. In the frequentist approach, we can use confidence interval, p-value, and different hypothesis testing methods to interpret and predict the parameter. However, no matter what you do, there is always one underlying assumption: the parameter theta is a fixed value. We may not, and as humans, may never know the real value of the parameter, but there is a perfect solution. In some way, this assumption is almost religious: we believe that our world is perfect, and that there is a way to express the underlying truth. Bayesian, however, thinks about this in a different way. Their assumption is that the parameter is a random variable. We, as humans, will be using our own knowledge, called priors, to model the distribution of this parameter. As we observe more and more data points, we continue to update our beliefs and propose a new distribution of the parameter. This process is called updating, and it is one of the reason why Bayesian’s approach is really powerful: You can update the distribution of the parameter whenever you receive new information of the parameter, without running the entire model again.\n",
    "\n",
    "The soul of Bayesian Analysis is the Bayes’ formula:\n",
    "$$\n",
    "p(\\theta|y) = \\frac{p(y|\\theta)\\times p(\\theta)}{p(y)}\n",
    "$$\n",
    "But in practice, we can often ignore p(y), since it is not a function of theta.\n",
    "So, the rule becomes something like this:\n",
    "$$\n",
    "p(\\theta|y) \\propto p(y|\\theta)\\times p(\\theta)\n",
    "$$\n",
    "Here, we call p(y|theta) \"likelihood\", and it represents our proposed model. p(theta) is our \"prior\", as aforementioned, this is our prior belief of the distribution of the parameter theta. In practice, this might be, say, the conclusion of another team's experiment. p(theta|y), then, is what we call \"posterior\". This is the result of our Bayesian Analysis, the distribution of the parameter we are interested in.\n",
    "There are a few special conjugate models that would give us a clean and easy close-form solution for the posterior distribution, for example, the famous beta-binomial model. I will attach a list of commonly used conjugate models after this section.\n",
    "However, if you are super familiar with statistical theories, you might already realize a huge problem with this approach: Most of the time,there are no close-form solutions for p(theta|y)! If you really, really want to try and calculate it, you are most likely facing a high dimensional intergral, which is almost never easy to calculate. This is actually one of the reasons why for the past hundreds of years, Bayesian statistics was never the mainstream. Before the invention of computers, calculating these kinds of high dimensional integrals are unpractical, and for the most part of human history, impossible. Hell, even after the invention of computers, these kind of intergrals might still take days to calculate! However, a sampling method came out and save these hardcore Bayesians: MCMC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "# For simplicity, let's consider a beta-binomial model.\n",
    "def likelihood(theta, data):\n",
    "    z = sum(data)\n",
    "    n = len(data)\n",
    "    pDataGivenTheta = pow(theta,z) * pow((1-theta), (n-z))\n",
    "    # The theta values passed into this function are generated at random, \n",
    "    # and therefore might be inadvertently greater than 1 or less than 0. \n",
    "    # The likelihood for theta > 1 or for theta < 0 is zero: \n",
    "    pDataGivenTheta[ theta > 1 | theta < 0 ] = 0 \n",
    "    return pDataGivenTheta\n",
    "\n",
    "def prior(theta):\n",
    "    pTheta = stats.beta(1, 1)\n",
    "    # The theta values passed into this function are generated at random, \n",
    "    # and therefore might be inadvertently greater than 1 or less than 0. \n",
    "    # The prior for theta > 1 or for theta < 0 is zero: \n",
    "    pTheta[ theta > 1 | theta < 0 ] = 0 \n",
    "    return( pTheta ) \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
